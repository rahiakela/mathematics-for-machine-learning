{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNQerpimVZxqVfXZ12O96Hk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/mathematics-for-machine-learning/blob/main/deep-learning-book-maths/2_12_principal_components_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Principal Components Analysis"
      ],
      "metadata": {
        "id": "IKtQpuOAWn8Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dimensions are a crucial topic in data science. The dimensions are all the features of the dataset. For instance, if you are looking at a dataset containing pieces of music, dimensions could be the genre, the length of the piece, the number of instruments, the presence of a singer etc. You can imagine all these dimensions as different columns. When there is only two dimensions, it is very convenient to plot: you can use the $x$- and $y$-axis. Add color and you can represent a third dimension. It is similar if you have tens or hundereds of dimensions, it will just be harder to visualize it.\n",
        "\n",
        "When you have that many dimensions it happens that some of them are correlated. For instance, we can reasonably think that the genre dimension will correlate with the instruments dimensions in our previous example. One way to reduce dimensionality is simply to keep only some of them. The problem is that you loose good information. It would be nice to have a way to reduce these dimensions while keeping all the information present in the data set.\n",
        "\n",
        "The aim of principal components analysis (PCA) is generaly to reduce the number of dimensions of a dataset where dimensions are not completly decorelated. PCA provides us with a new set of dimensions, the principal components (PC). They are ordered: the first PC is the dimension having the largest variance. In addition, each PC is orthogonal to the preceding one. Remember that orthogonal vectors means that their dot product is equal to $0$ (see [2.6](https://hadrienj.github.io/posts/Deep-Learning-Book-Series-2.6-Special-Kinds-of-Matrices-and-Vectors/)). This means that each PC is decorelated to the preceding one. It is way better than feature selection where you loose a lot of information."
      ],
      "metadata": {
        "id": "W-btWPbn-f86"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Setup"
      ],
      "metadata": {
        "id": "hu1fEM6AZFWr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "XMujFm6dZGgH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot style\n",
        "sns.set()\n",
        "%pylab inline\n",
        "pylab.rcParams['figure.figsize'] = (4, 4)"
      ],
      "metadata": {
        "id": "dI6fjg6qZG7p",
        "outputId": "98187766-2790-4244-918a-623855781266",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Populating the interactive namespace from numpy and matplotlib\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_vectors(vecs, cols, alpha=1):\n",
        "  \"\"\"\n",
        "  Plot set of vectors.\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  vecs : array-like\n",
        "      Coordinates of the vectors to plot. Each vectors is in an array. For\n",
        "      instance: [[1, 3], [2, 2]] can be used to plot 2 vectors.\n",
        "  cols : array-like\n",
        "      Colors of the vectors. For instance: ['red', 'blue'] will display the\n",
        "      first vector in red and the second in blue.\n",
        "  alpha : float\n",
        "      Opacity of vectors\n",
        "\n",
        "  Returns:\n",
        "\n",
        "  fig : instance of matplotlib.figure.Figure\n",
        "      The figure of the vectors\n",
        "  \"\"\"\n",
        "  plt.axvline(x=0, color='#A9A9A9', zorder=0)\n",
        "  plt.axhline(y=0, color='#A9A9A9', zorder=0)\n",
        "\n",
        "  for i in range(len(vecs)):\n",
        "      if (isinstance(alpha, list)):\n",
        "          alpha_i = alpha[i]\n",
        "      else:\n",
        "          alpha_i = alpha\n",
        "      x = np.concatenate([[0,0],vecs[i]])\n",
        "      plt.quiver([x[0]],\n",
        "                  [x[1]],\n",
        "                  [x[2]],\n",
        "                  [x[3]],\n",
        "                  angles='xy', scale_units='xy', scale=1, color=cols[i],\n",
        "                alpha=alpha_i)"
      ],
      "metadata": {
        "id": "LScJC8SNoWON"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Describing the problem"
      ],
      "metadata": {
        "id": "9Tbolg9LYAzH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Example 1: Orthogonal vector"
      ],
      "metadata": {
        "id": "R1cdq6K62RrL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unit vectors are an example of orthogonal vectors:\n",
        "\n",
        "<img src=\"https://github.com/rahiakela/mathematics-for-machine-learning/blob/main/deep-learning-book-maths/images/orthogonal-vectors.png?raw=1\" width=\"200\" alt=\"Example of orthogonal vectors\" title=\"Orthogonal vectors\">\n",
        "\n",
        "<em>Orthogonal vectors</em>"
      ],
      "metadata": {
        "id": "nQNsQZpSMOEx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Describing the problem"
      ],
      "metadata": {
        "id": "DfBv2oWrVCeZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The problem can be expressed as finding a function that converts a set of data points from $\\mathbb{R}^n$ to $\\mathbb{R}^l$. This means that we change the number of dimensions of our dataset. We also need a function that can decode back from the transformed dataset to the initial one:\n",
        "\n",
        "<img src=\"https://github.com/rahiakela/mathematics-for-machine-learning/blob/main/deep-learning-book-maths/images/principal-components-analysis-PCA-change-coordinates.png?raw=1\" width=\"80%\" alt=\"Principal components analysis (PCA)\" title=\"Principal components analysis (PCA)\">\n",
        "\n",
        "<em>Principal components analysis as a change of coordinate system</em>\n",
        "\n",
        "The first step is to understand the shape of the data. $x^{(i)}$ is one data point containing $n$ dimensions. \n",
        "\n",
        "Let's have $m$ data points organized as column vectors (one column per point):\n",
        "\n",
        "$$\n",
        "{x}=\n",
        "\\begin{bmatrix}\n",
        "    x^{(1)} & x^{(2)} & \\cdots & x^{(m)}\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "If we deploy the $n$ dimensions of our data points we will have:\n",
        "\n",
        "$$\n",
        "{x}=\n",
        "\\begin{bmatrix}\n",
        "    x_1^{(1)} & x_1^{(2)} & \\cdots & x_1^{(m)}\\\\\\\\\n",
        "    x_2^{(1)} & x_2^{(2)} & \\cdots & x_2^{(m)}\\\\\\\\\n",
        "    \\cdots & \\cdots & \\cdots & \\cdots\\\\\\\\\n",
        "    x_n^{(1)} & x_n^{(2)} & \\cdots & x_n^{(m)}\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "We can also write:\n",
        "\n",
        "$$\n",
        "{x}=\n",
        "\\begin{bmatrix}\n",
        "    x_1\\\\\\\\\n",
        "    x_2\\\\\\\\\n",
        "    \\cdots\\\\\\\\\n",
        "    x_n\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "$c$ will have the shape:\n",
        "\n",
        "$$\n",
        "{c}=\n",
        "\\begin{bmatrix}\n",
        "    c_1\\\\\\\\\n",
        "    c_2\\\\\\\\\n",
        "    \\cdots\\\\\\\\\n",
        "    c_l\n",
        "\\end{bmatrix}\n",
        "$$"
      ],
      "metadata": {
        "id": "VpZUp_43VDCL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Adding some constraints: the decoding function"
      ],
      "metadata": {
        "id": "p41-WrloVQuB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The encoding function $f({x})$ transforms ${x}$ into ${c}$ and the decoding function transforms back ${c}$ into an approximation of ${x}$. \n",
        "\n",
        "To keep things simple, PCA will respect some constraints:"
      ],
      "metadata": {
        "id": "uxjVDrIiVRQA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Constraint 1"
      ],
      "metadata": {
        "id": "i7AY8u75Vc8u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The decoding function has to be a simple matrix multiplication:\n",
        "\n",
        "$$\n",
        "g({c})={Dc}\n",
        "$$\n",
        "\n",
        "By applying the matrix ${D}$ to the dataset from the new coordinates system we should get back to the initial coordinate system."
      ],
      "metadata": {
        "id": "b8Q_c-l9Vhzj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Constraint 2"
      ],
      "metadata": {
        "id": "chIdyBzVVl4g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The columns of ${D}$ must be orthogonal (see [2.6](https://hadrienj.github.io/posts/Deep-Learning-Book-Series-2.6-Special-Kinds-of-Matrices-and-Vectors/))."
      ],
      "metadata": {
        "id": "J8PnueS1VoSS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Constraint 3"
      ],
      "metadata": {
        "id": "-Y73slqQVr3t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The columns of ${D}$ must have unit norm (see [2.6](https://hadrienj.github.io/posts/Deep-Learning-Book-Series-2.6-Special-Kinds-of-Matrices-and-Vectors/))."
      ],
      "metadata": {
        "id": "FZ12gd4qVt2X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Finding the encoding function"
      ],
      "metadata": {
        "id": "wCF9NKt1VzHD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Important: For now we will consider only **one data point**. Thus we will have the following dimensions for these matrices (note that ${x}$ and ${c}$ are column vectors):\n",
        "\n",
        "<img src=\"https://github.com/rahiakela/mathematics-for-machine-learning/blob/main/deep-learning-book-maths/images/principal-components-analysis-PCA-decoding-function.png?raw=1\" width=\"250\" alt=\"Principal components analysis (PCA) - the decoding function\" title=\"The decoding function\">\n",
        "<em>The decoding function</em>\n",
        "\n",
        "We want a decoding function which is a simple matrix multiplication. For that reason, we have $g({c})={Dc}$. We will then find the encoding function from the decoding function. We want to minimize the error between the decoded data point and the actual data point. With our previous notation, this means reducing the distance between ${x}$ and $g({c})$. As an indicator of this distance, we will use the squared $L^2$ norm (see [2.5](https://hadrienj.github.io/posts/Deep-Learning-Book-Series-2.5-Norms/)):\n",
        "\n",
        "$$\n",
        "||{{x} - g({c})}||_2^2\n",
        "$$\n",
        "\n",
        "This is what we want to minimize. Let's call ${c}^*$ the optimal ${c}$. Mathematically it can be written:\n",
        "\n",
        "$$\n",
        "{c}^* = \\underset{c}{\\arg\\min} ||{{x} - g({c})}||_2^2\n",
        "$$\n",
        "\n",
        "This means that we want to find the values of the vector ${c}$ such that $||{{x} - g({c})}||_2^2$ is as small as possible.\n",
        "\n",
        "If you have a look back to [2.5](https://hadrienj.github.io/posts/Deep-Learning-Book-Series-2.5-Norms/) you can see that the squared $L^2$ norm can be expressed as:\n",
        "\n",
        "$$\n",
        "||{{y}}||_2^2 = {y}^\\text{T}{y}\n",
        "$$\n",
        "\n",
        "We have named the variable ${y}$ to avoid confusion with our ${x}$. Here ${y}={x} - g({c})$\n",
        "\n",
        "\n",
        "Thus the equation that we want to minimize becomes:\n",
        "\n",
        "$$\n",
        "({x} - g({c}))^\\text{T}({x} - g({c}))\n",
        "$$\n",
        "\n",
        "Since the transpose respects addition we have:\n",
        "\n",
        "$$\n",
        "({x}^\\text{T} - g({c})^\\text{T})({x} - g({c}))\n",
        "$$\n",
        "\n",
        "By the distributive property (see [2.2](https://hadrienj.github.io/posts/Deep-Learning-Book-Series-2.2-Multiplying-Matrices-and-Vectors/)) we can develop:\n",
        "\n",
        "$$\n",
        "{x^\\text{T}x} - {x}^\\text{T}g({c}) -  g({c})^\\text{T}{x} + g({c})^\\text{T}g({c})\n",
        "$$\n",
        "\n",
        "The commutative property (see [2.2](https://hadrienj.github.io/posts/Deep-Learning-Book-Series-2.2-Multiplying-Matrices-and-Vectors/)) tells us that $\n",
        "{x^\\text{T}y} = {y^\\text{T}x}\n",
        "$. We can use that in the previous equation: we have $\n",
        "g({c})^\\text{T}{x} = {x}^\\text{T}g({c})\n",
        "$. So the equation becomes:\n",
        "\n",
        "$$\n",
        "{x^\\text{T}x} -{x}^\\text{T}g({c}) -{x}^\\text{T}g({c}) + g({c})^\\text{T}g({c})\\\\\\\\\n",
        "= {x^\\text{T}x} -2{x}^\\text{T}g({c}) + g({c})^\\text{T}g({c})\n",
        "$$\n",
        "\n",
        "The first term ${x^\\text{T}x}$ does not depends on ${c}$ and since we want to minimize the function according to ${c}$ we can just get off this term. We simplify to:\n",
        "\n",
        "$$\n",
        "{c}^* = \\underset{c}{\\arg\\min} -2{x}^\\text{T}g({c}) + g({c})^\\text{T}g({c})\n",
        "$$\n",
        "\n",
        "Since $g({c})={Dc}$:\n",
        "\n",
        "$$\n",
        "{c}^* = \\underset{c}{\\arg\\min} -2{x}^\\text{T}{Dc} + ({Dc})^\\text{T}{Dc}\n",
        "$$\n",
        "\n",
        "With $({Dc})^\\text{T}={c}^\\text{T}{D}^\\text{T}$ (see [2.2](https://hadrienj.github.io/posts/Deep-Learning-Book-Series-2.2-Multiplying-Matrices-and-Vectors/)), we have:\n",
        "\n",
        "$$\n",
        "{c}^* = \\underset{c}{\\arg\\min} -2{x}^\\text{T}{Dc} + {c}^\\text{T}{D}^\\text{T}{Dc}\n",
        "$$\n",
        "\n",
        "As we saw in [2.6](https://hadrienj.github.io/posts/Deep-Learning-Book-Series-2.6-Special-Kinds-of-Matrices-and-Vectors/), ${D}^\\text{T}{D}={I}_l$ because ${D}$ is orthogonal (actually, it is [semi-orthogonal](https://en.wikipedia.org/wiki/Semi-orthogonal_matrix) if $n \\neq l$) and their columns have unit norm. We can replace in the equation:\n",
        "\n",
        "$$\n",
        "{c}^* = \\underset{c}{\\arg\\min} -2{x}^\\text{T}{Dc} + {c}^\\text{T}{I}_l{c}\n",
        "$$\n",
        "\n",
        "$$\n",
        "{c}^* = \\underset{c}{\\arg\\min} -2{x}^\\text{T}{Dc} + {c}^\\text{T}{c}\n",
        "$$"
      ],
      "metadata": {
        "id": "p3DjnCQ2Vzl0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Minimizing the function"
      ],
      "metadata": {
        "id": "nl_LWSJwX5qa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "So far so good! Now the goal is to find the minimum of the function $- 2{x}^\\text{T}{Dc} + {c}^\\text{T}{c}$. One widely used way of doing that is to use the **gradient descent** algorithm. It is not the focus of this chapter but we will say a word about it (see [4.3](http://www.deeplearningbook.org/contents/numerical.html) of the Deep Learning Book for more details). The main idea is that the sign of the derivative of the function at a specific value of $x$ tells you if you need to increase or decrease $x$ to reach the minimum. When the slope is near $0$, the minimum should have been reached.\n",
        "\n",
        "<img src=\"https://github.com/rahiakela/mathematics-for-machine-learning/blob/main/deep-learning-book-maths/images/gradient-descent.png?raw=1\" width=\"400\" alt=\"Mechanism of the gradient descent algorithm\" title=\"Mechanism of the gradient descent algorithm\">\n",
        "<em>Gradient descent</em>\n",
        "\n",
        "However, functions with local minima can trouble the descent:\n",
        "\n",
        "<img src=\"https://github.com/rahiakela/mathematics-for-machine-learning/blob/main/deep-learning-book-maths/images/gradient-descent-local-minima.png?raw=1\" width=\"400\" alt=\"Gradient descent in the case of local minimum\" title=\"Gradient descent\">\n",
        "<em>Gradient descent can get stuck in local minima</em>\n",
        "\n",
        "These examples are in 2 dimensions but the principle stands for higher dimensional functions. The gradient is a vector containing the partial derivatives of all dimensions. Its mathematical notation is $\\nabla_xf({x})$."
      ],
      "metadata": {
        "id": "tX52686OX6LC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Calculating the gradient of the function"
      ],
      "metadata": {
        "id": "NuEQl0ZmYIxz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we want to minimize through each dimension of ${c}$. We are looking for a slope of $0$. The equation is:\n",
        "\n",
        "$$\n",
        "\\nabla_c(-2{x}^\\text{T}{Dc} + {c}^\\text{T}{c})=0\n",
        "$$\n",
        "\n",
        "Let's take these terms separately to calculate the derivative according to ${c}$. \n",
        "\n",
        "$$\n",
        "\\frac{d(-2{x}^\\text{T}{Dc})}{d{c}} = -2{x}^\\text{T}{D}\n",
        "$$\n",
        "\n",
        "The second term is ${c}^\\text{T}{c}$. We can develop the vector ${c}$ and calculate the derivative for each element:\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\frac{d({c}^\\text{T}{c})}{d{c}} &=\n",
        "\\left(\\frac{d({c}_1^2 + {c}_2^2 + \\cdots + {c}_l^2)}{d{c}_1},\n",
        "\\frac{d({c}_1^2 + {c}_2^2 + \\cdots + {c}_l^2)}{d{c}_2},\n",
        "\\cdots,\n",
        "\\frac{d({c}_1^2 + {c}_2^2 + \\cdots + {c}_l^2)}{d{c}_l}\\right) \\\\\\\\\n",
        "&=(2{c}_1, 2{c}_2, \\cdots, 2{c}_l)\\\\\\\\\n",
        "&=2({c}_1, {c}_2, \\cdots, {c}_l)\\\\\\\\\n",
        "&=2{c}\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "So we can progress in our derivatives:\n",
        "\n",
        "$$\n",
        "\\nabla_c(-2{x}^\\text{T}{Dc} + {c}^\\text{T}{c})=0\\\\\\\\\n",
        "-2{x}^\\text{T}{D} + 2{c}=0\\\\\\\\\n",
        "-2{D}^\\text{T}{x} + 2{c}=0\\\\\\\\\n",
        "{c}={D}^\\text{T}{x}\n",
        "$$\n",
        "\n",
        "Great! We found the encoding function! Here are its dimensions:\n",
        "\n",
        "<img src=\"https://github.com/rahiakela/mathematics-for-machine-learning/blob/main/deep-learning-book-maths/images/principal-components-analysis-PCA-encoding-function.png?raw=1\" width=\"250\" alt=\"Expression of the encoding function\" title=\"The encoding function\">\n",
        "<em>The encoding function</em>\n",
        "\n",
        "To go back from ${c}$ to ${x}$ we use $g({c})={Dc}$:\n",
        "\n",
        "$$\n",
        "r({x}) = g(f({x})={D}{D}^\\text{T}{x}\n",
        "$$\n",
        "\n",
        "<img src=\"https://github.com/rahiakela/mathematics-for-machine-learning/blob/main/deep-learning-book-maths/images/principal-components-analysis-PCA-reconstruction-function.png?raw=1\" width=\"300\" alt=\"Expression of the reconstruction function\" title=\"The reconstruction function\">\n",
        "<em>The reconstruction function</em>"
      ],
      "metadata": {
        "id": "JyQ-6e6PYJOv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## References"
      ],
      "metadata": {
        "id": "pGcdv0qwjACO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PCA**\n",
        "\n",
        "- A lot of intuitive explanations on PCA: https://arxiv.org/pdf/1404.1100.pdf\n",
        "\n",
        "- https://brilliant.org/wiki/principal-component-analysis/#from-approximate-equality-to-minimizing-function\n",
        "\n",
        "- http://www4.ncsu.edu/~slrace/LinearAlgebra2017/Slides/PCAPrint.pdf\n",
        "\n",
        "- https://towardsdatascience.com/a-one-stop-shop-for-principal-component-analysis-5582fb7e0a9c\n",
        "\n",
        "- https://www.cs.bgu.ac.il/~inabd171/wiki.files/lecture14_handouts.pdf\n",
        "\n",
        "**Semi-orthogonal matrix**\n",
        "\n",
        "- https://en.wikipedia.org/wiki/Semi-orthogonal_matrix\n",
        "\n",
        "**Intuition about PCA**\n",
        "\n",
        "- https://georgemdallas.wordpress.com/2013/10/30/principal-component-analysis-4-dummies-eigenvectors-eigenvalues-and-dimension-reduction/\n",
        "\n",
        "**Derivatives**\n",
        "\n",
        "- https://math.stackexchange.com/questions/1377764/derivative-of-vector-and-vector-transpose-product\n",
        "\n",
        "**Link between variance maximized and error minimized**:\n",
        "\n",
        "- https://stats.stackexchange.com/questions/130721/what-norm-of-the-reconstruction-error-is-minimized-by-the-low-rank-approximation\n",
        "\n",
        "- https://stats.stackexchange.com/questions/32174/pca-objective-function-what-is-the-connection-between-maximizing-variance-and-m\n",
        "\n",
        "- https://stats.stackexchange.com/questions/318625/why-do-the-leading-eigenvectors-of-a-maximize-texttrdtad\n",
        "\n",
        "**Centering data**\n",
        "\n",
        "- https://www.quora.com/Why-do-we-need-to-center-the-data-for-Principle-Components-Analysis\n",
        "- https://stats.stackexchange.com/questions/22329/how-does-centering-the-data-get-rid-of-the-intercept-in-regression-and-pca\n",
        "\n",
        "**Unit norm constraint**\n",
        "\n",
        "- https://stats.stackexchange.com/questions/117695/why-is-the-eigenvector-in-pca-taken-to-be-unit-norm"
      ],
      "metadata": {
        "id": "1vK_KJ_3jBKD"
      }
    }
  ]
}